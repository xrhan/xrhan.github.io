<!DOCTYPE html>
<html lang="en">

<style>
  /* MAIN CONTAINER */
  .video-row {
    display: flex;
    justify-content: center;  /* Centers the whole group on page */
    
    /* --- CONTROLLABLE MARGIN SETTINGS --- */
    gap: 3rem;                /* Change this to increase/decrease space between videos */
    width: 60%;               /* Set to 100% for full width, or less to shrink */
    /* ------------------------------------ */
    
    margin: 0 auto;           /* Centers the row horizontally */
  }

  /* COLUMNS */
  .video-col {
    flex: 1;                  /* Forces all 3 items to be exactly the same size */
    display: flex;
    flex-direction: column;   /* Stacks text on top of video */
  }

  /* LABELS */
  .video-label {
    text-align: center;
    font-size: 1.8rem;        /* Made font slightly larger */
    margin-bottom: 10px;
    font-family: Lato;
    color: #333;
  }

  /* CONTENT (Video/Img) */
  .video-box video,
  .video-box img {
    width: auto;
    height: clamp(100px, 12vh, 150px);
    object-fit: contain;
    display: block;
  }
</style>

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Generative Perception of Shape and Material from Differential Motion</title>
  <!-- Stylesheets -->
  <link rel="stylesheet" href="css/bootstrap.min.css">
  <link rel="stylesheet" href="css/app.css">
  <!-- Optional: Font Awesome -->
  <link rel="stylesheet" href="css/font-awesome.min.css">

  <script src="js/video_comparison.js"></script>
  <script src="js/app.js"></script>
</head>
<body>
  <div class="container text-center" id="header">
    <h1 id="paper-title">Generative Perception of Shape and Material<br> from Differential Motion<br>
                <small>
                    NeurIPS 2025
                </small></h1>
            <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="https://xrhan.github.io">
                              Xinran (Nicole) Han
                            </a>
                            <br>Harvard University
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://vision.ist.i.kyoto-u.ac.jp/">
                              Ko Nishino
                            </a>
                            <br>Kyoto University
                        </td>
                        <td>
                            <a style="text-decoration:none" href="http://www.eecs.harvard.edu/~zickler/Main/HomePage">
                                Todd Zickler
                            </a>
                            <br>Harvard University
                        </td>
                    </tr>
                </table>
            </div>
        </div>
  </div>

    <div class="container" id="main">
      <br>
        <div class="row">
                <div class="col-sm-6 col-sm-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://openreview.net/pdf/71ee2336c2f9c3274d08e92fd0ccd243cc1cea35.pdf">
                            <img src="./imgs/arxiv.png" height="20px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://neurips.cc/virtual/2025/loc/san-diego/poster/118348">
                            <img src="./imgs/NeurIPS_logo.png" height="20px">
                                <h4><strong>Slides & Presentation</strong></h4>
                            </a>
                        </li>                                               
                        <li>
                            <a href="https://github.com/xrhan/diffmotion">
                            <image src="imgs/github.png" height="20px">
                                <h4><strong>Code and Data</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>
  <div class="container">
  <div class="row">
            <div class="col-md-8 col-md-offset-2">
              <h3>
                    TL;DR
                </h3>
                We train a pixel-space diffusion model from scratch to jointly predict normals, albedo and reflectance from three-frame videos of object motion.
            </div>  
  </div>


  <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
Perceiving the shape and material of an object from a single image is inherently ambiguous, 
especially when lighting is unknown and unconstrained. Despite this, humans can often disentangle 
shape and material, and when they are uncertain, they often move their head slightly or rotate the 
object to help resolve the ambiguities. Inspired by this behavior, we introduce a novel conditional 
denoising-diffusion model that generates samples of shape-and-material maps from a short video of an 
object undergoing differential motions. Our parameter-efficient architecture allows training directly 
in pixel-space, and it generates many disentangled attributes of an object simultaneously. Trained on 
a modest number of synthetic object-motion videos with supervision on shape and material, the model exhibits 
compelling emergent behavior: For static observations, it produces diverse, multimodal predictions of plausible 
shape-and-material maps that capture the inherent ambiguities; and when objects move, the distributions converge 
to more accurate explanations. The model also produces high-quality shape-and-material estimates for less ambiguous, 
real-world objects. 
By moving beyond single-view to continuous motion observations, and by using generative perception to capture visual 
ambiguities, our work suggests ways to improve visual reasoning in physically-embodied systems.                </p>

                  <div class="text-center">
                    <img src="./imgs/setup.png" style="width:100%; max-width:550px; height:auto; display:block; margin:0 auto;">
                </div>
            </div>
        </div>

<div class="row">
  <div class="col-md-8 col-md-offset-2">
    <h3>Qualitative Comparison</h3>
Here we compare to DiffusionRenderer (CVPR 2025), another video-conditioned model that can output albedo and normals. 
Objects 5 (Teapot) and 6 (Cactus) are from the Stanford-ORB dataset (NeurIPS 2023); the input comes from irregular camera motion 
that is different from the horizontal object motion that we use for training.
<br>
<br>
<div id="comparison-widget">

  <!-- MAIN AREA: only one of these is visible at a time -->
  <div class="comparison-main">

    <!-- Set 1 -->
    <div class="video-compare-row comparison-set is-active"
         data-key="cloud">
      <!-- LEFT: input video -->
      <div class="video-col">
        <video class="video-media"
               loop playsinline muted autoplay
               src="video/cloud_input.mp4">
        </video>
      </div>

      <!-- MIDDLE: arrow -->
      <div class="arrow-col">
        <div class="arrow-horizontal"></div>
      </div>

      <!-- RIGHT: comparison widget -->
      <div class="video-col">
        <div class="video-compare-container">
          <video class="video-media"
                 id="materials"
                 loop playsinline muted autoplay
                 src="video/cloud_combined.mp4"
                 onplay="resizeAndPlay(this)">
          </video>
          <canvas class="video-media videoMerge" id="materialsMerge"></canvas>
        </div>
      </div>
    </div>

    <!-- Set 2 -->
    <div class="video-compare-row comparison-set"
         data-key="sculptures">
      <div class="video-col">
        <video class="video-media"
               loop playsinline muted autoplay
               src="video/two_sculptures_input.mp4">
        </video>
      </div>
      <div class="arrow-col">
        <div class="arrow-horizontal"></div>
      </div>
      <div class="video-col">
        <div class="video-compare-container">
          <video class="video-media"
                 id="materials2"
                 loop playsinline muted autoplay
                 src="video/two_sculptures_combined.mp4"
                 onplay="resizeAndPlay(this)">
          </video>
          <canvas class="video-media videoMerge" id="materials2Merge"></canvas>
        </div>
      </div>
    </div>

    <!-- Set 3 -->
    <div class="video-compare-row comparison-set"
         data-key="elephant">
      <div class="video-col">
        <video class="video-media"
               loop playsinline muted autoplay
               src="video/ours_elephant_input.mp4">
        </video>
      </div>
      <div class="arrow-col">
        <div class="arrow-horizontal"></div>
      </div>
      <div class="video-col">
        <div class="video-compare-container">
          <video class="video-media"
                 id="materials3"
                 loop playsinline muted autoplay
                 src="video/elephant_combined.mp4"
                 onplay="resizeAndPlay(this)">
          </video>
          <canvas class="video-media videoMerge" id="materials3Merge"></canvas>
        </div>
      </div>
    </div>

    <!-- Set 4 -->
    <div class="video-compare-row comparison-set"
         data-key="vase">
      <div class="video-col">
        <video class="video-media"
               loop playsinline muted autoplay
               src="video/ours_vase_input.mp4">
        </video>
      </div>
      <div class="arrow-col">
        <div class="arrow-horizontal"></div>
      </div>
      <div class="video-col">
        <div class="video-compare-container">
          <video class="video-media"
                 id="materials4"
                 loop playsinline muted autoplay
                 src="video/vase_combined.mp4"
                 onplay="resizeAndPlay(this)">
          </video>
          <canvas class="video-media videoMerge" id="materials4Merge"></canvas>
        </div>
      </div>
    </div>

    <!-- Set 5 -->
    <div class="video-compare-row comparison-set"
         data-key="teapot">
      <div class="video-col">
        <video class="video-media"
               loop playsinline muted autoplay
               src="video/teapot_input.mp4">
        </video>
      </div>
      <div class="arrow-col">
        <div class="arrow-horizontal"></div>
      </div>
      <div class="video-col">
        <div class="video-compare-container">
          <video class="video-media"
                 id="materials5"
                 loop playsinline muted autoplay
                 src="video/teapot_combined.mp4"
                 onplay="resizeAndPlay(this)">
          </video>
          <canvas class="video-media videoMerge" id="materials5Merge"></canvas>
        </div>
      </div>
    </div>

    <!-- Set 6 -->
    <div class="video-compare-row comparison-set"
         data-key="cactus">
      <div class="video-col">
        <video class="video-media"
               loop playsinline muted autoplay
               src="video/cactus_input.mp4">
        </video>
      </div>
      <div class="arrow-col">
        <div class="arrow-horizontal"></div>
      </div>
      <div class="video-col">
        <div class="video-compare-container">
          <video class="video-media"
                 id="materials6"
                 loop playsinline muted autoplay
                 src="video/cactus_combined.mp4"
                 onplay="resizeAndPlay(this)">
          </video>
          <canvas class="video-media videoMerge" id="materials6Merge"></canvas>
        </div>
      </div>
    </div>
  </div>

  <!-- THUMB STRIP: scrollable selector using left videos as snapshots -->
  <div class="comparison-thumbs">
    <button class="comparison-thumb is-active" data-target="cloud">
      <video class="thumb-video" muted playsinline preload="metadata"
             src="video/cloud_input.mp4"></video>
      <span>1. Cloud</span>
    </button>

    <button class="comparison-thumb" data-target="sculptures">
      <video class="thumb-video" muted playsinline preload="metadata"
             src="video/two_sculptures_input.mp4"></video>
      <span>2. Sculptures</span>
    </button>

    <button class="comparison-thumb" data-target="elephant">
      <video class="thumb-video" muted playsinline preload="metadata"
             src="video/ours_elephant_input.mp4"></video>
      <span>3. Elephant</span>
    </button>

        <button class="comparison-thumb" data-target="vase">
      <video class="thumb-video" muted playsinline preload="metadata"
             src="video/ours_vase_input.mp4"></video>
      <span>4. Vase</span>
    </button>

    <button class="comparison-thumb" data-target="teapot">
      <video class="thumb-video" muted playsinline preload="metadata"
             src="video/teapot_input.mp4"></video>
      <span>5. Teapot</span>
    </button>

    <button class="comparison-thumb" data-target="cactus">
      <video class="thumb-video" muted playsinline preload="metadata"
             src="video/cactus_input.mp4"></video>
      <span>6. Cactus</span>
    </button>
  </div>
</div>

  
  </div>
</div>

<div class="row"> 
  <div class="col-md-8 col-md-offset-2" id="video-section">
    <h3>Extending to Longer Observations</h3>
    <!-- Video Placeholder 1 -->
    <div class="text-justify">
    We generate longer videos of shape and material by jointly predicting several three-frame clips that overlap and are regularized by inference-time consistency guidance.
    We compare with the shape prediction result from StableNormal, using their official video mode demo.<br><br>
    </div>
    <!-- ROW 1: videos 1 & 2 -->
    <div class="video-rowrow">
      <!-- First video with split caption -->
      <figure class="video-item">
        <!-- Split caption bar under the video -->
        <div class="split-caption">
          <div class="split-caption-left">Input</div>
          <div class="split-caption-right">Ours - normal and albedo</div>
        </div>
        <video
          class="video"
          id="l1"
          loop
          playsinline
          autoplay
          muted
          src="video/long_dance_fps10.mp4"
          onplay="resizeAndPlay(this)"
        ></video>
      </figure>

      <!-- Second video with single, centered caption -->
      <figure class="video-item">
        <div class="video-simple-caption">StableNormal</div>
        <video
          class="video"
          id="l2"
          loop
          playsinline
          autoplay
          muted
          src="video/sn_dance.mp4"
          onplay="resizeAndPlay(this)"
        ></video>
      </figure>
    </div>

    <!-- ROW 2: videos 3 & 4 -->
    <div class="video-rowrow">
      <figure class="video-item">
        <video
          class="video"
          id="l3"
          loop
          playsinline
          autoplay
          muted
          src="video/long_sphere_fps10.mp4"
          onplay="resizeAndPlay(this)"
        ></video>
        <!-- Split caption bar under the video -->
      </figure>

      <figure class="video-item">
        <video
          class="video"
          id="l4"
          loop
          playsinline
          autoplay
          muted
          src="video/sn_sphere.mp4"
          onplay="resizeAndPlay(this)"
        ></video>
      </figure>
    </div>

</div>
</div>

          <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Capturing Ambiguity
                </h3>
                <div class="text-justify">
When input is ambiguous, our model captures the ambiguity by generating diverse samples of plausible shape-and-material combinations. 
Here, our model correctly infers that a static image of a diffuse object can be explained in at least three different ways.
<br>
</div>
<div class="text-center">
                    <img src="./imgs/static_sfs.png" style="width:100%; max-width:520px; height:auto; display:block; margin:0 auto;">
                </div>
                
            <h3>
              Reducing Ambiguity by Exploiting Motion
            </h3>

            There is usually less ambiguity about shape and material in a video than in a static image. 
            A good example is Hartung and Kersten's <a href="https://kerstenlab.psych.umn.edu/demos/motion-surface/" target="_blank" rel="noopener">
            "Shiny or Matte" perceptual demonstration </a>, shown here. 
            The still image can be seen as either shiny or matte material, but once the object moves, we immediately perceive either one or the other.
            <br>
            
            <div class="video-row">
            <div class="video-col">
              <div class="video-label">Static</div>
              <div class="video-box">
                <img src="./imgs/pot_static.png" alt="Static Result">
              </div>
            </div>

            <div class="video-col">
              <div class="video-label">Diffuse</div>
              <div class="video-box">
                <video src="./video/pot_vid_diffuse.mp4" controls muted loop></video>
              </div>
            </div>

            <div class="video-col">
              <div class="video-label">Shiny</div>
              <div class="video-box">
                <video src="./video/pot_vid_shiny.mp4" controls muted loop></video>
              </div>
            </div>
          </div>

            <br>
            Our model behaves the same way. When it sees the static image, it produces a variety of albedo maps, including both matte and shiny interpretations. 
            And when it sees the shiny motion video, its albedo predictions collapse to a smaller set of spatially-uniform possibilities. 
            (The right figure shows a PCA-embedding of 100 albedo-map samples for each of the static and motion cases.)
          <br>
                <br>
                <div class="text-center">
                          <video
        class="video" id="panel" loop playsinline autoplay muted src="video/pot_panel.m4v" 
        style="
        display:block;
        margin:0 auto;
        width:100%;
        max-width:700px;
        height:auto;
        object-fit:contain;" onplay="resizeAndPlay(this)"></video>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Model Architecture
                </h3>
                <div class="text-justify">
                    Our parameter-efficient denoising network, U-ViT3D-Mixer, takes in a channel-wise concatenation 
                    of conditional video frames and noisy shape-and-material frames. At high spatial resolutions, 
                    it uses efficient local 3D blocks (middle) with decoupled spatial, temporal, and channel-wise 
                    interactions. At lower spatial resolutions, it uses global transformer layers with full 3D attention.
                </div>
                <br>
                <div class="text-center">
                    <img src="./imgs/network_v2.png" width="90%">
                </div>
            </div>
        </div>

  <!-- Results Visualization -->
   <div class="row">
  <div class="col-md-8 col-md-offset-2" id="results-visualization">
    <h3>Results Visualizations</h3>
      <div class="text-justify">
      We visualize the input video sequence, predicted surface normal and albedo from a moving object. 
      Input videos are synthetically rendered using a physics-based rendering engine (1,2) or are captured real world objects (3).<br><br>
      (1) Objects with synthetic textures:<br>
    </div>
    <!-- Row of 3 videos -->
    <div class="row">
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e1" loop playsinline autoPlay muted src="video/cloud.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e2" loop playsinline autoPlay muted src="video/piggy.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e3" loop playsinline autoPlay muted src="video/plant.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
    </div>

    <!-- Add more <div class="row"> blocks if you have additional videos -->
        <!-- Row of 3 videos -->
      <div class="text-justify">
      (2) Objects with original, artists' designed textures:<br>
    </div>
    <div class="row">
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e4" loop playsinline autoPlay muted src="video/honeyjar.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e5" loop playsinline autoPlay muted src="video/cookies.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e6" loop playsinline autoPlay muted src="video/spoon.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
    </div>

    (3) Captured real world object-motion videos:<br>
    <!-- Row of 3 videos -->
    <div class="row">
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e1" loop playsinline autoPlay muted src="video/ducky.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e2" loop playsinline autoPlay muted src="video/ours_blue_vase.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e3" loop playsinline autoPlay muted src="video/ours_frother.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
    </div>
        
</div>
</div>

<div class="row" id="BibTeX">
  <div class="col-md-8 col-md-offset-2">
    <h3>BibTeX</h3>
    <pre><code>@article{han2025generative,
  title={Generative Perception of Shape and Material from Differential Motion},
  author={Han, Xinran Nicole and Nishino, Ko and Zickler, Todd},
  journal={arXiv preprint arXiv:2506.02473},
  year={2025}
}</code></pre>
  </div>
</div>

<div class="row">
    <div class="col-md-8 col-md-offset-2">
        <h3>
            Acknowledgements
        </h3>
        <p class="text-justify">
          We thank Jianbo Shi for reviewing the manuscript. We also thank Kohei Yamashita for guidance about the data rendering pipeline and Boyuan Chen for discussion 
          about video diffusion modeling. This work was supported in part by the NSF cooperative agreement PHY-2019786 (an NSF AI Institute, <a href="http://iaifi.org">http://iaifi.org</a>) 
          and by JSPS 21H04893 and JST JPMJAP2305.
        <br><br>
        The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a>.
        </p>
    </div>
</div>
</div>
  <!-- Optional: Scripts -->
  <script src="js/jquery.min.js"></script>
  <script src="js/bootstrap.min.js"></script>
  <script src="js/app.js"></script>

<script>
document.addEventListener('DOMContentLoaded', () => {
  const sets   = document.querySelectorAll('.comparison-set');
  const thumbs = document.querySelectorAll('.comparison-thumb');

  function activateSet(key) {
    // Toggle visible set
    sets.forEach(set => {
      const isTarget = set.dataset.key === key;
      set.classList.toggle('is-active', isTarget);
    });

    // Update thumb styles
    thumbs.forEach(btn => {
      btn.classList.toggle('is-active', btn.dataset.target === key);
    });
  }

thumbs.forEach(btn => {
  btn.addEventListener('click', () => {
    const key = btn.dataset.target;
    activateSet(key);
  });

  const tv = btn.querySelector('.thumb-video');
  if (tv) {
    tv.addEventListener('loadedmetadata', () => {
      // Seek to a tiny offset and pause -> browser renders a frame
      tv.currentTime = 0.001;
      tv.pause();
    }, { once: true });

    // Start a tiny play just to allow seeking, then we'll pause
    tv.play().then(() => {
      // We'll pause in the loadedmetadata handler after seek
    }).catch(() => {
      // If autoplay is blocked, you may still get just the first frame after metadata
    });
  }
});

  // Initial activation (in case no is-active class, default to first)
  const initial = document.querySelector('.comparison-set.is-active') ||
                  sets[0];
  if (initial) {
    activateSet(initial.dataset.key);
  }
});

</script>

</body>
</html>

