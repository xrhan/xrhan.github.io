<!DOCTYPE html>
<html lang="en">

<style>
  .video-row {
    display: flex;
    justify-content: space-between;
    gap: 5rem;                /* space between the two videos */
    width: 55%;              /* <-- limit the entire row to 75% of page */
    margin: 0 auto; 
  }
  .video-box {
    width: 40%;               /* or any % / px you like */
    padding-top: 40%;         /* makes the box a square (1:1 aspect) */
    position: relative;
    background: #000;         /* letter-box bars will match this */
  }
  .video-box video {
    position: absolute;
    top: 50%; left: 50%;
    /* make the video fill the container as much as possible without cropping */
    max-width: 100%;
    max-height: 100%;
    transform: translate(-50%, -50%);
    object-fit: contain;
  }
</style>

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Generative Perception of Shape and Material from Differential Motion</title>
  <!-- Stylesheets -->
  <link rel="stylesheet" href="css/bootstrap.min.css">
  <link rel="stylesheet" href="css/app.css">
  <!-- Optional: Font Awesome -->
  <link rel="stylesheet" href="css/font-awesome.min.css">

  <script src="js/video_comparison.js"></script>
  <script src="js/app.js"></script>
</head>
<body>
  <div class="container text-center" id="header">
    <h1 id="paper-title">Generative Perception of Shape and Material<br> from Differential Motion<br>
                <small>
                    NeurIPS 2025
                </small></h1>
            <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="https://xrhan.github.io">
                              Xinran (Nicole) Han
                            </a>
                            <br>Harvard University
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://vision.ist.i.kyoto-u.ac.jp/">
                              Ko Nishino
                            </a>
                            <br>Kyoto University
                        </td>
                        <td>
                            <a style="text-decoration:none" href="http://www.eecs.harvard.edu/~zickler/Main/HomePage">
                                Todd Zickler
                            </a>
                            <br>Harvard University
                        </td>
                    </tr>
                </table>
            </div>
        </div>
  </div>

    <div class="container" id="main">
      <br>
        <div class="row">
                <div class="col-sm-6 col-sm-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://openreview.net/pdf/71ee2336c2f9c3274d08e92fd0ccd243cc1cea35.pdf">
                            <img src="./imgs/arxiv.png" height="20px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://recorder-v3.slideslive.com/?share=106246&s=5383ea60-5151-4fa9-b576-5da84c192911">
                            <img src="./imgs/NeurIPS_logo.png" height="20px">
                                <h4><strong>Slides & Presentation</strong></h4>
                            </a>
                        </li>                                               
                        <li>
                            <a href="https://github.com/xrhan/diffmotion">
                            <image src="imgs/github.png" height="20px">
                                <h4><strong>Code and Data</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>
  <div class="container">
  <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
Perceiving the shape and material of an object from a single image is inherently ambiguous, 
especially when lighting is unknown and unconstrained. Despite this, humans can often disentangle 
shape and material, and when they are uncertain, they often move their head slightly or rotate the 
object to help resolve the ambiguities. Inspired by this behavior, we introduce a novel conditional 
denoising-diffusion model that generates samples of shape-and-material maps from a short video of an 
object undergoing differential motions. Our parameter-efficient architecture allows training directly 
in pixel-space, and it generates many disentangled attributes of an object simultaneously. Trained on 
a modest number of synthetic object-motion videos with supervision on shape and material, the model exhibits 
compelling emergent behavior: For static observations, it produces diverse, multimodal predictions of plausible 
shape-and-material maps that capture the inherent ambiguities; and when objects move, the distributions converge 
to more accurate explanations. The model also produces high-quality shape-and-material estimates for less ambiguous, 
real-world objects. 
By moving beyond single-view to continuous motion observations, and by using generative perception to capture visual 
ambiguities, our work suggests ways to improve visual reasoning in physically-embodied systems.                </p>

                  <div class="text-center">
                    <img src="./imgs/setup.png" width="65%">
                </div>
            </div>
        </div>

<div class="row">
  <div class="col-md-8 col-md-offset-2">
    <h3>Qualitative Comparison</h3>
We compare our model with another video-conditioned baseline - DiffusionRenderer (CVPR 2025) on object centric short videos (three unique frames).
on a diverse set of synthetic and real videos. The last two objects come from the Stanford-ORB (NeurIPS 2023) dataset.
<br>
<br>
<div id="comparison-widget">

  <!-- MAIN AREA: only one of these is visible at a time -->
  <div class="comparison-main">

    <!-- Set 1 -->
    <div class="video-compare-row comparison-set is-active"
         data-key="cloud">
      <!-- LEFT: input video -->
      <div class="video-col">
        <video class="video-media"
               loop playsinline muted autoplay
               src="video/cloud_input.mp4">
        </video>
      </div>

      <!-- MIDDLE: arrow -->
      <div class="arrow-col">
        <div class="arrow-horizontal"></div>
      </div>

      <!-- RIGHT: comparison widget -->
      <div class="video-col">
        <div class="video-compare-container">
          <video class="video-media"
                 id="materials"
                 loop playsinline muted autoplay
                 src="video/cloud_side_by_side.mp4"
                 onplay="resizeAndPlay(this)">
          </video>
          <canvas class="video-media videoMerge" id="materialsMerge"></canvas>
        </div>
      </div>
    </div>

    <!-- Set 2 -->
    <div class="video-compare-row comparison-set"
         data-key="sculptures">
      <div class="video-col">
        <video class="video-media"
               loop playsinline muted autoplay
               src="video/two_sculptures_input.mp4">
        </video>
      </div>
      <div class="arrow-col">
        <div class="arrow-horizontal"></div>
      </div>
      <div class="video-col">
        <div class="video-compare-container">
          <video class="video-media"
                 id="materials2"
                 loop playsinline muted autoplay
                 src="video/two_sculptures_side_by_side.mp4"
                 onplay="resizeAndPlay(this)">
          </video>
          <canvas class="video-media videoMerge" id="materials2Merge"></canvas>
        </div>
      </div>
    </div>

    <!-- Set 3 -->
    <div class="video-compare-row comparison-set"
         data-key="elephant">
      <div class="video-col">
        <video class="video-media"
               loop playsinline muted autoplay
               src="video/ours_elephant_input.mp4">
        </video>
      </div>
      <div class="arrow-col">
        <div class="arrow-horizontal"></div>
      </div>
      <div class="video-col">
        <div class="video-compare-container">
          <video class="video-media"
                 id="materials3"
                 loop playsinline muted autoplay
                 src="video/elephant_side_by_side.mp4"
                 onplay="resizeAndPlay(this)">
          </video>
          <canvas class="video-media videoMerge" id="materials3Merge"></canvas>
        </div>
      </div>
    </div>

    <!-- Set 4 -->
    <div class="video-compare-row comparison-set"
         data-key="vase">
      <div class="video-col">
        <video class="video-media"
               loop playsinline muted autoplay
               src="video/ours_vase_input.mp4">
        </video>
      </div>
      <div class="arrow-col">
        <div class="arrow-horizontal"></div>
      </div>
      <div class="video-col">
        <div class="video-compare-container">
          <video class="video-media"
                 id="materials4"
                 loop playsinline muted autoplay
                 src="video/vase_side_by_side.mp4"
                 onplay="resizeAndPlay(this)">
          </video>
          <canvas class="video-media videoMerge" id="materials4Merge"></canvas>
        </div>
      </div>
    </div>

    <!-- Set 5 -->
    <div class="video-compare-row comparison-set"
         data-key="teapot">
      <div class="video-col">
        <video class="video-media"
               loop playsinline muted autoplay
               src="video/teapot_input.mp4">
        </video>
      </div>
      <div class="arrow-col">
        <div class="arrow-horizontal"></div>
      </div>
      <div class="video-col">
        <div class="video-compare-container">
          <video class="video-media"
                 id="materials5"
                 loop playsinline muted autoplay
                 src="video/teapot_side_by_side.mp4"
                 onplay="resizeAndPlay(this)">
          </video>
          <canvas class="video-media videoMerge" id="materials5Merge"></canvas>
        </div>
      </div>
    </div>

    <!-- Set 6 -->
    <div class="video-compare-row comparison-set"
         data-key="cactus">
      <div class="video-col">
        <video class="video-media"
               loop playsinline muted autoplay
               src="video/cactus_input.mp4">
        </video>
      </div>
      <div class="arrow-col">
        <div class="arrow-horizontal"></div>
      </div>
      <div class="video-col">
        <div class="video-compare-container">
          <video class="video-media"
                 id="materials6"
                 loop playsinline muted autoplay
                 src="video/cactus_side_by_side.mp4"
                 onplay="resizeAndPlay(this)">
          </video>
          <canvas class="video-media videoMerge" id="materials6Merge"></canvas>
        </div>
      </div>
    </div>

  </div>

  <!-- THUMB STRIP: scrollable selector using left videos as snapshots -->
  <div class="comparison-thumbs">
    <button class="comparison-thumb is-active" data-target="cloud">
      <video class="thumb-video" muted playsinline preload="metadata"
             src="video/cloud_input.mp4"></video>
      <span>1. Cloud</span>
    </button>

    <button class="comparison-thumb" data-target="sculptures">
      <video class="thumb-video" muted playsinline preload="metadata"
             src="video/two_sculptures_input.mp4"></video>
      <span>2. Sculptures</span>
    </button>

    <button class="comparison-thumb" data-target="elephant">
      <video class="thumb-video" muted playsinline preload="metadata"
             src="video/ours_elephant_input.mp4"></video>
      <span>3. Elephant</span>
    </button>

        <button class="comparison-thumb" data-target="vase">
      <video class="thumb-video" muted playsinline preload="metadata"
             src="video/ours_vase_input.mp4"></video>
      <span>4. Vase</span>
    </button>

    <button class="comparison-thumb" data-target="teapot">
      <video class="thumb-video" muted playsinline preload="metadata"
             src="video/teapot_input.mp4"></video>
      <span>5. Teapot</span>
    </button>

    <button class="comparison-thumb" data-target="cactus">
      <video class="thumb-video" muted playsinline preload="metadata"
             src="video/cactus_input.mp4"></video>
      <span>6. Cactus</span>
    </button>
  </div>
</div>

  
  </div>
</div>

<div class="row"> 
  <div class="col-md-8 col-md-offset-2" id="video-section">
    <h3>Extending to Longer Observations</h3>
    <!-- Video Placeholder 1 -->
    <div class="text-justify">
    We apply temporal consistency guidance on the predictions of the overlapping frame to extend our 
    model to longer video sequence. 
    This guided sampling method allows temporally consistent shape and material estimation. 
    We compare with the shape prediction result from StableNormal, using their official video mode demo.<br><br>
    </div>

    <!-- ROW 1: videos 1 & 2 -->
    <div class="video-rowrow">
      <figure class="video-item">
        <video
          class="video"
          id="l1"
          loop
          playsinline
          autoplay
          muted
          src="video/long_dance_fps10.mp4"
          onplay="resizeAndPlay(this)"
        ></video>
        <figcaption>Ours</figcaption>
      </figure>

      <figure class="video-item">
        <video
          class="video"
          id="l2"
          loop
          playsinline
          autoplay
          muted
          src="video/sn_dance.mp4"
          onplay="resizeAndPlay(this)"
        ></video>
        <figcaption>StableNormal</figcaption>
      </figure>
    </div>

    <!-- ROW 2: videos 3 & 4 -->
    <div class="video-rowrow">
      <figure class="video-item">
        <video
          class="video"
          id="l3"
          loop
          playsinline
          autoplay
          muted
          src="video/long_sphere_fps10.mp4"
          onplay="resizeAndPlay(this)"
        ></video>
        <figcaption>Ours</figcaption>
      </figure>

      <figure class="video-item">
        <video
          class="video"
          id="l4"
          loop
          playsinline
          autoplay
          muted
          src="video/sn_sphere.mp4"
          onplay="resizeAndPlay(this)"
        ></video>
        <figcaption>StableNormal</figcaption>
      </figure>
    </div>

</div>
</div>

          <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Motivation - Interdependence & Ambiguity of Shape and Material
                </h3>
                <div class="text-justify">
Different combinations of shapes, materials, and lighting, can give the exact same image so recovering them 
from the image is inherently ambiguous. At the same time, given observations, different physical attributes 
are NOT mutually independent and ought to be disentangled jointly. 
<br>
</div>
<div class="text-center">
                    <img src="./imgs/static_sfs.png" width="55%">
                </div>
                <br>
                <b>Static Image Example</b>: Our model shows an emergent ability to provide multiple hypotheses for ambiguous static images, 
                such as convex, concave and planar `postcard' explanations.

            <h4>
              Shiny or Matte Ambiguity
            (by
            <a href="https://kerstenlab.psych.umn.edu/demos/motion-surface/" target="_blank" rel="noopener">
              Hartung &amp; Kersten
            </a>
            )</h4>
            <div class="video-row">
              <div class="video-box">
                <video src="./video/perception1.mp4" controls></video>
              </div>
              <div class="video-box">
                <video src="./video/perception2.mp4" controls></video>
              </div>
            </div>
            <br>
            <b>Video Examples</b>: An object (such as the teapot or the crossiant shape above), when viewed as a static observation, can be ambiguous. 
            Once we see the object in motion, we can often easily perceive the underlying material of the object. In both videos, 
            the object is first rendered with a shiny surface reflecting the environment, and then switches to a diffuse object with painted texture.
          <br>
          <br>
          <b>Prior work</b>: Despite this inherent and fundamental ambiguity 
of an image, most computer vision systems aim for a single 
“best” explanation. Deterministic models, or methods that smooth over samples, tend to suppress ambiguity 
rather than represent it. And by predicting shape or material in isolation, those models overlook other 
plausible combinations of physical estimates.  
          
          </div>
        </div>

<div class="row">
<div class="col-md-8 col-md-offset-2">       
<h4>
    Our approach - Generative Perception
</h4>
We argue that vision systems 
should <strong>embrace and model ambiguities</strong> in shape and material, so that other cues like motion or touch 
can be leveraged when needed for disambiguation. We advocate a <strong>generative perception</strong> approach, where 
the vision system generates diverse samples of shapes-and-materials that explain the object appearance captured in the input observations.
<br>
        </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Model Architecture
                </h3>
                <div class="text-justify">
                    Our parameter-efficient denoising network, U-ViT3D-Mixer, takes in a channel-wise concatenation 
                    of conditional video frames and noisy shape-and-material frames. At high spatial resolutions, 
                    it uses efficient local 3D blocks (middle) with decoupled spatial, temporal, and channel-wise 
                    interactions. At lower spatial resolutions, it uses global transformer layers with full 3D attention.
                </div>
                <br>
                <div class="text-center">
                    <img src="./imgs/network_v2.png" width="90%">
                </div>
            </div>
        </div>


  <!-- Results Visualization -->
   <div class="row">
  <div class="col-md-8 col-md-offset-2" id="results-visualization">
    <h3>Results Visualizations</h3>
      <div class="text-justify">
      We visualize the input video sequence, predicted surface normal and albedo (diffuse color) from a moving object. 
      Input videos are synthetically rendered using physics-based rendering engine (1,2) or captured real world objects (3).<br><br>
      (1) Objects with synthetic textures:<br><br>
    </div>
    <!-- Row of 3 videos -->
    <div class="row">
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e1" loop playsinline autoPlay muted src="video/cloud.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e2" loop playsinline autoPlay muted src="video/piggy.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e3" loop playsinline autoPlay muted src="video/plant.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
    </div>

    <!-- Add more <div class="row"> blocks if you have additional videos -->
        <!-- Row of 3 videos -->
      <div class="text-justify">
      <br> (2) Objects with original, artists' designed textures:<br><br>
    </div>
    <div class="row">
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e4" loop playsinline autoPlay muted src="video/honeyjar.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e5" loop playsinline autoPlay muted src="video/cookies.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e6" loop playsinline autoPlay muted src="video/spoon.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
    </div>

      (3) Captured real-world objects motion videos:<br><br>
    <!-- Row of 3 videos -->
    <div class="row">
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e1" loop playsinline autoPlay muted src="video/ducky.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e2" loop playsinline autoPlay muted src="video/ours_blue_vase.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e3" loop playsinline autoPlay muted src="video/ours_frother.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
    </div>

  </div>
  </div>
  
  
  
  <div class="row"> 
  <div class="col-md-8 col-md-offset-2" id="video-section">
    <h3>Motion Helps Disambiguation</h3>
    <!-- Video Placeholder 1 -->
    <div class="text-justify">
    Visualization of the shiny/matte perception test designed by Hartung and Kersten. Note in the croissant shape example the static frames can look similar for both the shiny and diffuse material. Motion cues help to disambiguate the materials.<br><br>
    </div>
<div class="row">
  <div class="col-md-4 video-wrapper text-center">
    <figure style="margin:0;">
      <video
        class="video" id="p1" loop playsinline autoplay muted src="video/pot_shiny.mp4" onplay="resizeAndPlay(this)"
        style="height:75px; width:auto; object-fit:contain; display:block;"></video>
      <figcaption style="margin-top:.5rem; font-size:1.4rem; color:#666;">
        Pot (shiny)
      </figcaption>
    </figure>
  </div>
  <div class="col-md-4 video-wrapper text-center">
    <figure style="margin:0;">
      <video
        class="video" id="p2" loop playsinline autoplay muted src="video/croissant_shiny.mp4" onplay="resizeAndPlay(this)"
        style="height:75px; width:auto; object-fit:contain; display:block;"></video>
      <figcaption style="margin-top:.5rem; font-size:1.4rem; color:#666;">
        Croissant (shiny)
      </figcaption>
    </figure>
  </div>
  <div class="col-md-4 video-wrapper text-center">
    <figure style="margin:0;">
      <video
        class="video" id="p3" loop playsinline autoplay muted src="video/croissant_diffuse.mp4" onplay="resizeAndPlay(this)"
        style="height:75px; width:auto; object-fit:contain; display:block;"
      ></video>
      <figcaption style="margin-top:.5rem; font-size:1.4rem; color:#666;">
        Croissant (diffuse)
      </figcaption>
    </figure>
  </div>
  </div>
        <div class="row">
                <br>
                <div class="text-center">
                    <img src="./imgs/motion_disambiguation.png" width="90%">
                </div>
                <br>
                <b>Motion Disambiguation Analysis</b>: Each plot shows the 2D PCA embedding of 100 albedo samples 
                corresponding to the frame (inset) that is common between two distinct input videos. 
                <i>Left:</i> When a shiny teapot moves, the albedo samples converge to a <b>subset</b>
                of the ones produced for the static teapot. <i>Right:</i> The albedo samples for a 
                matte-rendered motion video are clearly <b>separated</b> from those of a shiny-rendered video. 
                Note that spatially-uniform albedos form tighter clusters in PCA space, while highly textured 
                ones exhibit greater variation.
            </div>
        </div>
</div>
</div>

<div class="row" id="BibTeX">
  <div class="col-md-8 col-md-offset-2">
    <h3>BibTeX</h3>
    <pre><code>@article{han2025generative,
  title={Generative Perception of Shape and Material from Differential Motion},
  author={Han, Xinran Nicole and Nishino, Ko and Zickler, Todd},
  journal={arXiv preprint arXiv:2506.02473},
  year={2025}
}</code></pre>
  </div>
</div>

<div class="row">
    <div class="col-md-8 col-md-offset-2">
        <h3>
            Acknowledgements
        </h3>
        <p class="text-justify">
          We thank Jianbo Shi for reviewing the manuscript. We also thank Kohei Yamashita for guidance about the data rendering pipeline and Boyuan Chen for discussion 
          about video diffusion modeling. This work was supported in part by the NSF cooperative agreement PHY-2019786 (an NSF AI Institute, <a href="http://iaifi.org">http://iaifi.org</a>) 
          and by JSPS 21H04893 and JST JPMJAP2305.
        <br>
        The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
        </p>
    </div>
</div>
</div>
  <!-- Optional: Scripts -->
  <script src="js/jquery.min.js"></script>
  <script src="js/bootstrap.min.js"></script>
  <script src="js/app.js"></script>

<script>
document.addEventListener('DOMContentLoaded', () => {
  const sets   = document.querySelectorAll('.comparison-set');
  const thumbs = document.querySelectorAll('.comparison-thumb');

  function activateSet(key) {
    // Toggle visible set
    sets.forEach(set => {
      const isTarget = set.dataset.key === key;
      set.classList.toggle('is-active', isTarget);
    });

    // Update thumb styles
    thumbs.forEach(btn => {
      btn.classList.toggle('is-active', btn.dataset.target === key);
    });
  }

thumbs.forEach(btn => {
  btn.addEventListener('click', () => {
    const key = btn.dataset.target;
    activateSet(key);
  });

  const tv = btn.querySelector('.thumb-video');
  if (tv) {
    tv.addEventListener('loadedmetadata', () => {
      // Seek to a tiny offset and pause -> browser renders a frame
      tv.currentTime = 0.001;
      tv.pause();
    }, { once: true });

    // Start a tiny play just to allow seeking, then we'll pause
    tv.play().then(() => {
      // We'll pause in the loadedmetadata handler after seek
    }).catch(() => {
      // If autoplay is blocked, you may still get just the first frame after metadata
    });
  }
});

  // Initial activation (in case no is-active class, default to first)
  const initial = document.querySelector('.comparison-set.is-active') ||
                  sets[0];
  if (initial) {
    activateSet(initial.dataset.key);
  }
});

</script>

</body>
</html>

