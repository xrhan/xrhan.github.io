<!DOCTYPE html>
<html lang="en">

<style>
  .video-row {
    display: flex;
    justify-content: space-between;
    gap: 5rem;                /* space between the two videos */
    width: 55%;              /* <-- limit the entire row to 75% of page */
    margin: 0 auto; 
  }
  .video-box {
    width: 45%;               /* or any % / px you like */
    padding-top: 45%;         /* makes the box a square (1:1 aspect) */
    position: relative;
    background: #000;         /* letter-box bars will match this */
  }
  .video-box video {
    position: absolute;
    top: 50%; left: 50%;
    /* make the video fill the container as much as possible without cropping */
    max-width: 100%;
    max-height: 100%;
    transform: translate(-50%, -50%);
    object-fit: contain;
  }
</style>

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Generative Perception of Shape and Material from Differential Motion</title>
  <!-- Stylesheets -->
  <link rel="stylesheet" href="css/bootstrap.min.css">
  <link rel="stylesheet" href="css/app.css">
  <!-- Optional: Font Awesome -->
  <link rel="stylesheet" href="css/font-awesome.min.css">
</head>
<body>
  <div class="container text-center" id="header">
    <h1 id="paper-title">Generative Perception of Shape and Material<br> from Differential Motion<br>
                <small>
                    Preprint
                </small></h1>
            <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="https://xrhan.github.io">
                              Xinran (Nicole) Han
                            </a>
                            <br>Harvard University
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://vision.ist.i.kyoto-u.ac.jp/">
                              Ko Nishino
                            </a>
                            <br>Kyoto University
                        </td>
                        <td>
                            <a style="text-decoration:none" href="http://www.eecs.harvard.edu/~zickler/Main/HomePage">
                                Todd Zickler
                            </a>
                            <br>Harvard University
                        </td>
                    </tr>
                </table>
            </div>
        </div>
  </div>

    <div class="container" id="main">
      <br>
        <div class="row">
                <div class="col-sm-6 col-sm-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="http://arxiv.org/abs/2506.02473">
                            <img src="./imgs/arxiv.png" height="20px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>                    
                        <li>
                            <a href="" target="_blank">
                            <image src="imgs/github.png" height="20px">
                                <h4><strong>Code and Data (coming soon) </strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>
  <div class="container">
  <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
Perceiving the shape and material of an object from a single image is inherently ambiguous, 
especially when lighting is unknown and unconstrained. Despite this, humans can often disentangle 
shape and material, and when they are uncertain, they often move their head slightly or rotate the 
object to help resolve the ambiguities. Inspired by this behavior, we introduce a novel conditional 
denoising-diffusion model that generates samples of shape-and-material maps from a short video of an 
object undergoing differential motions. Our parameter-efficient architecture allows training directly 
in pixel-space, and it generates many disentangled attributes of an object simultaneously. Trained on 
a modest number of synthetic object-motion videos with supervision on shape and material, the model 
exhibits compelling emergent behavior: For static observations, it produces diverse, multimodal 
predictions of plausible shape-and-material maps that capture the inherent ambiguities; and when 
objects move, the distributions quickly converge to more accurate explanations. The model also 
produces high-quality shape-and-material estimates for less ambiguous, real-world objects. By moving 
beyond single-view to continuous motion observations, our work suggests a generative perception approach 
for improving visual reasoning in physically-embodied systems.                </p>

                  <div class="text-center">
                    <img src="./imgs/setup_figure_v2.png" width="85%">
                </div>
            </div>
        </div>

          <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Interdependence of Shape and Material
                </h3>
                <div class="text-justify">
Interpreting visual inputs requires joint reasoning about shape, texture and reflectance. 
A static observation of an object can always be explained by a planar shape that has suitable paint or pigment 
(a `postcard' explanation). It can also be explained by mirror-like shapes that reflect suitable lighting. 
The set of possibilities is extensive and profoundly ambiguous, but in all of them there is an interplay 
between shape and material. This suggests that shape and material, which we represent here using 
reflectance and texture, ought to be inferred simultaneously. We argue that vision systems 
should embrace and model ambiguities in shape and material, so that other cues like motion or touch 
can be leveraged when needed for disambiguation. </div>
                <br>
                <div class="text-center">
                    <img src="./imgs/cat_carpet_ai.png" width="65%">
                </div>
                <br>
                <b>Static Object Examples</b>: Multiple shape and material hypotheses on ambiguous online images. 
                (a) A cat on an illusion carpet that is painted to resemble a hole. 
                When interpreted as a hole (Sample 1), the predicted albedo is brighter 
                in the center region (yellow box) than when interpreted as a plane (Sample 2). 
                (b) A painting of a pumpkin is interpreted as either a 3D shape with orange color 
                or a planar shape with painted texture. 

            <h4>
              Shiny or Matte Ambiguity
            (by
            <a href="https://kerstenlab.psych.umn.edu/demos/motion-surface/" target="_blank" rel="noopener">
              Hartung &amp; Kersten
            </a>
            )</h4>
            <div class="video-row">
              <div class="video-box">
                <video src="./video/perception1.mp4" controls></video>
              </div>
              <div class="video-box">
                <video src="./video/perception2.mp4" controls></video>
              </div>
            </div>
            <br>
            <b>Moving Object Examples</b>: An object (such as the teapot or the crossiant shape above), when viewed as a static observation, can be ambiguous. 
            Once we see the object in motion, we can often easily perceive the underlying material of the object. In both videos, 
            the object is first rendered with a shiny surface reflecting the environment, and then switches to a diffuse object with painted texture.
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Why Generative Perception?
                </h3>
                <div class="text-justify">
We advocate a generative perception approach, where the vision system generates diverse 
samples of shapes-and-materials that explain the object appearance captured in the input observations. 
When the input observation is a single static image, the output samples are expected to be diverse and 
express ambiguities. Moreover, perception should not be limited to a single view or a single attribute. 
It should naturally incorporate multiple views as they become available and jointly disentangle the 
complete physical attributes of object appearance--namely shape, texture, and reflectance--all at the same time. 
                </div>
                <br>
                <div class="text-center">
                    <img src="./imgs/motion_disambiguation.png" width="90%">
                </div>
                <br>
                <b>Motion Disambiguation Examples</b>: Each plot shows the 2D PCA embedding of 100 albedo samples 
                corresponding to the frame (inset) that is common between two distinct input videos. 
                <i>Left:</i> When a shiny teapot moves, the albedo samples converge to a <b>subset</b>
                of the ones produced for the static teapot. <i>Right:</i> The albedo samples for a 
                matte-rendered motion video are clearly <b>separated</b> from those of a shiny-rendered video. 
                Note that spatially-uniform albedos form tighter clusters in PCA space, while highly textured 
                ones exhibit greater variation.
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Model Architecture
                </h3>
                <div class="text-justify">
                    Our parameter-efficient denoising network, U-ViT3D-Mixer, takes in a channel-wise concatenation 
                    of conditional video frames and noisy shape-and-material frames. At high spatial resolutions, 
                    it uses efficient local 3D blocks (middle) with decoupled spatial, temporal, and channel-wise 
                    interactions. At lower spatial resolutions, it uses global transformer layers with full 3D attention.
                </div>
                <br>
                <div class="text-center">
                    <img src="./imgs/network_v2.png" width="90%">
                </div>
            </div>
        </div>


  <!-- Results Visualization -->
   <div class="row">
  <div class="col-md-8 col-md-offset-2" id="results-visualization">
    <h3>Results Visualizations</h3>
      <div class="text-justify">
      We visualize the input video sequence, predicted surface normal and albedo (diffuse color) from a moving object. 
      Input videos are synthetically rendered using physics-based rendering engine (1,2) or captured real world objects (3).<br><br>
      (1) Objects with synthetic textures:<br><br>
    </div>
    <!-- Row of 3 videos -->
    <div class="row">
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e1" loop playsinline autoPlay muted src="video/cloud.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e2" loop playsinline autoPlay muted src="video/piggy.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e3" loop playsinline autoPlay muted src="video/plant.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
    </div>

    <!-- Add more <div class="row"> blocks if you have additional videos -->
        <!-- Row of 3 videos -->
      <div class="text-justify">
      <br> (2) Objects with original, artists' designed textures:<br><br>
    </div>
    <div class="row">
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e4" loop playsinline autoPlay muted src="video/honeyjar.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e5" loop playsinline autoPlay muted src="video/cookies.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e6" loop playsinline autoPlay muted src="video/spoon.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
    </div>

      (3) Captured real-world objects motion videos:<br><br>
    </div>
    <!-- Row of 3 videos -->
    <div class="row">
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e1" loop playsinline autoPlay muted src="video/ducky.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e2" loop playsinline autoPlay muted src="video/elephant.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
      <div class="col-md-4 video-wrapper text-center">
        <video class="video" id="e3" loop playsinline autoPlay muted src="video/vase.mp4" onplay="resizeAndPlay(this)"></video>
      </div>
    </div>

  </div>
  </div>
  
  
  
  <div class="row"> 
  <div class="col-md-8 col-md-offset-2" id="video-section">
    <h3>Motion Disambiguation - Perception Test</h3>
    <!-- Video Placeholder 1 -->
    <div class="text-justify">
    Visualization of the shiny/matte perception test designed by Hartung and Kersten. Note in the croissant shape example the static frames can look similar for both the shiny and diffuse material. Motion cues help to disambiguate the materials.<br><br>
    </div>
<div class="row">
  <div class="col-md-4 video-wrapper text-center">
    <figure style="margin:0;">
      <video
        class="video" id="p1" loop playsinline autoplay muted src="video/pot_shiny.mp4" onplay="resizeAndPlay(this)"
        style="height:75px; width:auto; object-fit:contain; display:block;"></video>
      <figcaption style="margin-top:.5rem; font-size:1.4rem; color:#666;">
        Pot (shiny)
      </figcaption>
    </figure>
  </div>
  <div class="col-md-4 video-wrapper text-center">
    <figure style="margin:0;">
      <video
        class="video" id="p2" loop playsinline autoplay muted src="video/croissant_shiny.mp4" onplay="resizeAndPlay(this)"
        style="height:75px; width:auto; object-fit:contain; display:block;"></video>
      <figcaption style="margin-top:.5rem; font-size:1.4rem; color:#666;">
        Croissant (shiny)
      </figcaption>
    </figure>
  </div>
  <div class="col-md-4 video-wrapper text-center">
    <figure style="margin:0;">
      <video
        class="video" id="p3" loop playsinline autoplay muted src="video/croissant_diffuse.mp4" onplay="resizeAndPlay(this)"
        style="height:75px; width:auto; object-fit:contain; display:block;"
      ></video>
      <figcaption style="margin-top:.5rem; font-size:1.4rem; color:#666;">
        Croissant (diffuse)
      </figcaption>
    </figure>
  </div>
</div>
</div>
</div>

<div class="row"> 
  <div class="col-md-8 col-md-offset-2" id="video-section">
    <h3>Extending to Longer Observations</h3>
    <!-- Video Placeholder 1 -->
    <div class="text-justify">
    We apply temporal consistency guidance on the predictions of the overlapping frame to extend our 
    model to longer video sequence. 
    This guided sampling method allows temporally consistent shape and material estimation. 
    We compare with the shape prediction result from StableNormal, using their official video mode demo.<br><br>
    </div>

  <div class="row" style="display:flex; gap:1rem;">
    <!-- 1st video: flex 3 -->
    <div class="col-md-4 video-wrapper text-center" style="flex:3;">
      <figure style="margin:0;">
        <video
          class="video"
          id="l1"
          loop
          playsinline
          autoplay
          muted
          src="video/long_dance_fps10.mp4"
          onplay="resizeAndPlay(this)"
          style="height:78px; width:auto; object-fit:contain; display:block;"
        ></video>
        <figcaption style="margin-top:.5rem; font-size:1.4rem; color:#666;">
          Ours
        </figcaption>
      </figure>
    </div>

    <!-- 2nd video: flex 1 -->
    <div class="col-md-4 video-wrapper text-center" style="flex:1;">
      <figure style="margin:0;">
        <video
          class="video"
          id="l2"
          loop
          playsinline
          autoplay
          muted
          src="video/sn_dance.mp4"
          onplay="resizeAndPlay(this)"
          style="height:78px; width:auto; object-fit:contain; display:block;"
        ></video>
        <figcaption style="margin-top:.5rem; font-size:1.4rem; color:#666;">
          StableNormal
        </figcaption>
      </figure>
    </div>

    <!-- 3rd video: flex 3 -->
    <div class="col-md-4 video-wrapper text-center" style="flex:3;">
      <figure style="margin:0;">
        <video
          class="video"
          id="l3"
          loop
          playsinline
          autoplay
          muted
          src="video/long_sphere_fps10.mp4"
          onplay="resizeAndPlay(this)"
          style="height:78px; width:auto; object-fit:contain; display:block;"
        ></video>
        <figcaption style="margin-top:.5rem; font-size:1.4rem; color:#666;">
          Ours
        </figcaption>
      </figure>
    </div>

    <!-- 4th video: flex 1 -->
    <div class="col-md-4 video-wrapper text-center" style="flex:1;">
      <figure style="margin:0;">
        <video
          class="video"
          id="l4"
          loop
          playsinline
          autoplay
          muted
          src="video/sn_sphere.mp4"
          onplay="resizeAndPlay(this)"
          style="height:78px; width:auto; object-fit:contain; display:block;"
        ></video>
        <figcaption style="margin-top:.5rem; font-size:1.4rem; color:#666;">
          StableNormal
        </figcaption>
      </figure>
    </div>
  </div>
</div>
</div>

<div class="row">
    <div class="col-md-8 col-md-offset-2">
        <h3>
            Acknowledgements
        </h3>
        <p class="text-justify">
          We thank Jianbo Shi for reviewing the manuscript. We also thank Kohei Yamashita for guidance about the data rendering pipeline and Boyuan Chen for discussion 
          about video diffusion modeling. This work was supported in part by the NSF cooperative agreement PHY-2019786 (an NSF AI Institute, <a href="http://iaifi.org">http://iaifi.org</a>) 
          and by JSPS 21H04893 and JST JPMJAP2305.
        <br>
        The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
        </p>
    </div>
</div>
</div>
  <!-- Optional: Scripts -->
  <script src="js/jquery.min.js"></script>
  <script src="js/bootstrap.min.js"></script>
  <script src="js/app.js"></script>
</body>
</html>

